---
layout: article
title: vLLM使用调研
tags: vLLM LLM AI
key: 2024-08-23-vLLM-usage-research
---

## 背景

[vLLM](https://github.com/vllm-project/vllm)是一个为LLM服务的推理和管理工具  

## 版本

当前撰写版本是0.5.4  

## 文档

- 使用: <https://docs.vllm.ai/en/latest/getting_started/quickstart.html>  
- OpenAI兼容文档: <https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html>  

## 安装使用

```shell
# 直接使用pip安装
# 1. 安装最新release版
pip install vllm
# 2. 安装nightly版本
export VLLM_VERSION=0.5.4
pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-${VLLM_VERSION}-cp38-abi3-manylinux1_x86_64.whl

# pip安装后使用
vllm serve /path/to/model
# 直接使用vllm代码库运行
python -m vllm.entrypoints.openai.api_server --model /path/to/model
```

## 常用参数

参数可以见上面提到的OpenAI兼容文档，常见的有:  

```shell
vllm serve /data/models/qwen2/Qwen2-7B-Instruct
# 鉴权用的api key
--api-key token-abc123
# 监听端口
--port 9090
# 暴露给外部的vllm模型名
--served-model-name qwen2-7b
# 使用几个gpu跑 默认1个
--tensor-parallel-size 2
# gpu内存占用率 默认是0.9比较高 一般都需要改一下 避免挤压其他进程
--gpu-memory-utilization 0.5
# 模型上下文长度 如果不指定则从模型配置中获取
# 由于上面占用率配置的原因可能会导致上下文过长从而内存不足的问题 需配合配置
--max-model-len 45024
# 日志级别
--uvicorn-log-level trace
# 对话模板 不指定则使用默认模板
# 可以从huggingface上的tokenizer_config.json上获取默认模板 再去做修改 比较方便
# 例如https://huggingface.co/Qwen/Qwen2-7B-Instruct/blob/main/tokenizer_config.json中的chat_template字段
# 记得需要把对应的换行符等等去掉，格式化成一个正常的文本
--chat-template /data/models/qwen2/qwen2-7b.jinja
```

## embedding支持

vLLM目前仅支持arch为MistralModel的embedding模型。具体vLLM对不同种类的模型分别支持什么arch，可以看对应[vLLM配置](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/__init__.py)  
至于模型的arch是什么，可以看huggingface配置文件的config.json  
例如[intfloat/e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct/blob/main/config.json)就是MistralModel的arch  

## GGUF支持

当前版本尚未支持GGUF，需要使用master代码或者nightly版本的包  
例如使用llama-3.1-8b的模型，可以按如下:  

```shell
vllm serve /data/models/llama3.1-gguf/llama31-8b.gguf \
--api-key token-abc123 \
--port 9090 \
--served-model-name llama318b \
--tensor-parallel-size 2 \
--gpu-memory-utilization 0.5 \
--max-model-len 45024 \
--chat-template /data/models/llama3.1-gguf/llama31.jinja \
--uvicorn-log-level trace
```

对应模板为(从hf上获取的默认模板做了格式化): [vLLM-gguf-template.jinja]({{ site.baseurl }}/assets/text/vLLM-gguf-template.jinja)

## FUNCION CALL支持

当前版本基本不支持:  

- tool_choice只能指定单一的tool，而不能使用`auto`或者`required`选项  
- tool function call返回的消息无法作为`{"role": "tool"}`的格式加入prompt中  

有期望支持的PR，观望吧: <https://github.com/vllm-project/vllm/pull/3237>  

## 参考

- <https://github.com/vllm-project/vllm/issues/601>  
- <https://github.com/vllm-project/vllm/issues/5704>  
