---
layout: article
title: vLLM使用调研
tags: vLLM LLM AI
key: 2024-08-23-vLLM-usage-research
---

## 背景

[vLLM](https://github.com/vllm-project/vllm)是一个为LLM服务的推理和管理工具  

## 版本

当前撰写版本是0.5.4  

## 文档

- 使用: <https://docs.vllm.ai/en/latest/getting_started/quickstart.html>  
- OpenAI兼容文档: <https://docs.vllm.ai/en/latest/serving/openai_compatible_server.html>  

## 安装使用

```shell
# 直接使用pip安装
# 1. 安装最新release版
pip install vllm
# 2. 安装nightly版本
export VLLM_VERSION=0.5.4
pip install https://vllm-wheels.s3.us-west-2.amazonaws.com/nightly/vllm-${VLLM_VERSION}-cp38-abi3-manylinux1_x86_64.whl

# pip安装后使用
vllm serve /path/to/model
# 直接使用vllm代码库运行
python -m vllm.entrypoints.openai.api_server --model /path/to/model
```

## 常用参数

参数可以见上面提到的OpenAI兼容文档，常见的有:  

```shell
vllm serve /data/models/qwen2/Qwen2-7B-Instruct
# 鉴权用的api key
--api-key token-abc123
# 监听端口
--port 9090
# 暴露给外部的vllm模型名
--served-model-name qwen2-7b
# 使用几个gpu跑 默认1个
--tensor-parallel-size 2
# gpu内存占用率 默认是0.9比较高 一般都需要改一下 避免挤压其他进程
--gpu-memory-utilization 0.5
# 模型上下文长度 如果不指定则从模型配置中获取
# 由于上面占用率配置的原因可能会导致上下文过长从而内存不足的问题 需配合配置
--max-model-len 45024
# 日志级别
--uvicorn-log-level trace
# 对话模板 不指定则使用默认模板
# 可以从huggingface上的tokenizer_config.json上获取默认模板 再去做修改 比较方便
# 例如https://huggingface.co/Qwen/Qwen2-7B-Instruct/blob/main/tokenizer_config.json中的chat_template字段
# 记得需要把对应的换行符等等去掉，格式化成一个正常的文本
--chat-template /data/models/qwen2/qwen2-7b.jinja
```

## embedding支持

vLLM目前仅支持arch为MistralModel的embedding模型。具体vLLM对不同种类的模型分别支持什么arch，可以看对应[vLLM配置](https://github.com/vllm-project/vllm/blob/main/vllm/model_executor/models/__init__.py)  
至于模型的arch是什么，可以看huggingface配置文件的config.json  
例如[intfloat/e5-mistral-7b-instruct](https://huggingface.co/intfloat/e5-mistral-7b-instruct/blob/main/config.json)就是MistralModel的arch  

## GGUF支持

当前版本尚未支持GGUF，需要使用master代码或者nightly版本的包  
例如使用llama-3.1-8b的模型，可以按如下:  

```shell
vllm serve /data/models/llama3.1-gguf/llama31-8b.gguf \
--api-key token-abc123 \
--port 9090 \
--served-model-name llama318b \
--tensor-parallel-size 2 \
--gpu-memory-utilization 0.5 \
--max-model-len 45024 \
--chat-template /data/models/llama3.1-gguf/llama31.jinja \
--uvicorn-log-level trace
```

对应模板为(从hf上获取的默认模板做了格式化):
```jinja
{{- bos_token -}}
{%- if custom_tools is defined %}
    {%- set tools = custom_tools %}
{%- endif %}
{%- if not tools_in_user_message is defined %}
    {%- set tools_in_user_message = true %}
{%- endif %}
{%- if not date_string is defined %}
    {%- set date_string = "26 Jul 2024" %}
{%- endif %}
{%- if not tools is defined %}
    {%- set tools = none %}
{%- endif %}

{#- This block extracts the system message, so we can slot it into the right place. #}
{%- if messages[0]['role'] == 'system' %}
    {%- set system_message = messages[0]['content']|trim %}
    {%- set messages = messages[1:] %}
{%- else %}
    {%- set system_message = "" %}
{%- endif %}

{#- System message + builtin tools #}
{{- "<|start_header_id|>system<|end_header_id|>\n\n" }}
{%- if builtin_tools is defined or tools is not none %}
    {{- "Environment: ipython\n" }}
{%- endif %}
{%- if builtin_tools is defined %}
    {{- "Tools: " + builtin_tools | reject('equalto', 'code_interpreter') | join(", ") + "\n\n"}}
{%- endif %}
{{- "Cutting Knowledge Date: December 2023\n" }}
{{- "Today Date: " + date_string + "\n\n" }}
{%- if tools is not none and not tools_in_user_message %}
    {{- "You have access to the following functions. To call a function, please respond with JSON for a function call." }}
    {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.' }}
    {{- "Do not use variables.\n\n" }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- "\n\n" }}
    {%- endfor %}
{%- endif %}
{{- system_message }}
{{- "<|eot_id|>" }}

{#- Custom tools are passed in a user message with some extra guidance #}
{%- if tools_in_user_message and not tools is none %}
    {#- Extract the first user message so we can plug it in here #}
    {%- if messages | length != 0 %}
        {%- set first_user_message = messages[0]['content']|trim %}
        {%- set messages = messages[1:] %}
    {%- else %}
        {{- raise_exception("Cannot put tools in the first user message when there's no first user message!") }}
    {%- endif %}
    {{- '<|start_header_id|>user<|end_header_id|>\n\n' -}}
    {{- "Given the following functions, please respond with a JSON for a function call " }}
    {{- "with its proper arguments that best answers the given prompt.\n\n" }}
    {{- 'Respond in the format {"name": function name, "parameters": dictionary of argument name and its value}.' }}
    {{- "Do not use variables.\n\n" }}
    {%- for t in tools %}
        {{- t | tojson(indent=4) }}
        {{- "\n\n" }}
    {%- endfor %}
    {{- first_user_message + "<|eot_id|>"}}
{%- endif %}

{%- for message in messages %}
    {%- if not (message.role == 'ipython' or message.role == 'tool' or 'tool_calls' in message) %}
        {{- '<|start_header_id|>' + message['role'] + '<|end_header_id|>\n\n'+ message['content'] | trim + '<|eot_id|>' }}
    {%- elif 'tool_calls' in message %}
        {%- if not message.tool_calls|length == 1 %}
            {{- raise_exception("This model only supports single tool-calls at once!") }}
        {%- endif %}
        {%- set tool_call = message.tool_calls[0].function %}
        {%- if builtin_tools is defined and tool_call.name in builtin_tools %}
            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
            {{- "<|python_tag|>" + tool_call.name + ".call(" }}
            {%- for arg_name, arg_val in tool_call.arguments | items %}
                {{- arg_name + '="' + arg_val + '"' }}
                {%- if not loop.last %}
                    {{- ", " }}
                {%- endif %}
            {%- endfor %}
            {{- ")" }}
        {%- else  %}
            {{- '<|start_header_id|>assistant<|end_header_id|>\n\n' -}}
            {{- '{"name": "' + tool_call.name + '", ' }}
            {{- '"parameters": ' }}
            {{- tool_call.arguments | tojson }}
            {{- "}" }}
        {%- endif %}
        {%- if builtin_tools is defined %}
            {#- This means we're in ipython mode #}
            {{- "<|eom_id|>" }}
        {%- else %}
            {{- "<|eot_id|>" }}
        {%- endif %}
    {%- elif message.role == "tool" or message.role == "ipython" %}
        {{- "<|start_header_id|>ipython<|end_header_id|>\n\n" }}
        {%- if message.content is mapping or message.content is iterable %}
            {{- message.content | tojson }}
        {%- else %}
            {{- message.content }}
        {%- endif %}
        {{- "<|eot_id|>" }}
    {%- endif %}
{%- endfor %}

{%- if add_generation_prompt %}
    {{- "<|start_header_id|>assistant<|end_header_id|>\n\n" }}
{%- endif %}
```

## FUNCION CALL支持

当前版本基本不支持:  

- tool_choice只能指定单一的tool，而不能使用`auto`或者`required`选项  
- tool function call返回的消息无法作为`{"role": "tool"}`的格式加入prompt中  

有期望支持的PR，观望吧: <https://github.com/vllm-project/vllm/pull/3237>  

## 参考

- <https://github.com/vllm-project/vllm/issues/601>  
- <https://github.com/vllm-project/vllm/issues/5704>  
